## ğŸ”¹ What is Time Complexity?

* **Time complexity** tells us **how fast an algorithm runs** as the **input size grows**.
* Itâ€™s measured in terms of **number of operations relative to input size** (usually `n`).
* Python itself doesnâ€™t calculate it automatically â€” itâ€™s a **theoretical concept**, but important for writing efficient code.


## ğŸ”¹ Big O Notation

We usually use **Big O notation** to describe time complexity:

| Complexity | Meaning                             | Example                                  |
| ---------- | ----------------------------------- | ---------------------------------------- |
| O(1)       | Constant time â€“ doesnâ€™t grow with n | Accessing a list element `lst[5]`        |
| O(n)       | Linear time â€“ grows with n          | Looping through a list `for x in lst`    |
| O(n^2)     | Quadratic â€“ grows with square of n  | Nested loops (2 loops inside each other) |
| O(log n)   | Logarithmic â€“ halves each step      | Binary search                            |
| O(n log n) | Linearithmic                        | Efficient sorting (`sorted()` in Python) |


## ğŸ”¹ Python Examples

### 1ï¸âƒ£ Constant Time: O(1)

```python
lst = [1, 2, 3, 4, 5]
print(lst[2])  # Accessing element â†’ O(1)
```

### 2ï¸âƒ£ Linear Time: O(n)

```python
lst = [1, 2, 3, 4, 5]
for x in lst:
    print(x)  # Looping through all elements â†’ O(n)


### 3ï¸âƒ£ Quadratic Time: O(n^2)


lst = [1, 2, 3]
for i in lst:
    for j in lst:
        print(i, j)  # Nested loops â†’ O(n^2)


### Example with a `break` Condition

lst = [1, 2, 3]
target = 2  # Condition: stop when i == 2 and j == 2

for i in lst:
    for j in lst:
        print(i, j)
        if i == target and j == target:
            print("Condition met â†’ Breaking out of inner loop")
            break  # stops only inner loop



### **Execution**

ğŸ”¹ **Iteration flow:**

* `(1, 1)` â†’ print
* `(1, 2)` â†’ print
* `(1, 3)` â†’ print
* `(2, 1)` â†’ print
* `(2, 2)` â†’ condition met â†’ break inner loop
* Outer loop continues with `i = 3`, inner loop restarts.

---

### **If you want to break completely (stop both loops):**

```python
lst = [1, 2, 3]
target = 2
found = False

for i in lst:
    for j in lst:
        print(i, j)
        if i == target and j == target:
            print("Condition met â†’ Breaking out of both loops")
            found = True
            break
    if found:
        break
```


### **Best / Average / Worst Case Analysis**

| Case             | What Happens                                                    | Time Complexity        |
| ---------------- | --------------------------------------------------------------- | ---------------------- |
| **Best Case**    | Condition is met at very first pair `(1,1)` â†’ Break immediately | **O(1)**               |
| **Average Case** | Condition is met somewhere in the middle                        | **O(nÂ² / 2)** (approx) |
| **Worst Case**   | Condition never met â†’ Visit all pairs `(i, j)`                  | **O(nÂ²)**              |

---

âœ… **This is how you get different best, average, and worst cases.**
Without a `break`, we always get `O(nÂ²)`.

### 4ï¸âƒ£ Logarithmic Time: O(log n)

* Example: **Binary search**

def binary_search(arr, target):
    low = 0
    high = len(arr) - 1
    while low <= high:
        mid = (low + high) // 2
        if arr[mid] == target:
            return mid
        elif arr[mid] < target:
            low = mid + 1
        else:
            high = mid - 1
    return -1

* Each step **halves the array** â†’ O(log n)


## ğŸ”¹ What is O(n log n)?

* Itâ€™s called **linearithmic time complexity**.
* The **log n** part comes from **halving the problem repeatedly** (like in binary search or merge sort).
* The **n** part comes from **processing all elements at each step**.

**So basically:**

> â€œDo something to all n elements at each level of log n steps.â€



## ğŸ”¹ Example: Merge Sort (O(n log n))

def merge_sort(arr):
    if len(arr) <= 1:
        return arr

    mid = len(arr) // 2
    left = merge_sort(arr[:mid])
    right = merge_sort(arr[mid:])

    # Merge step
    result = []
    i = j = 0
    while i < len(left) and j < len(right):
        if left[i] < right[j]:
            result.append(left[i])
            i += 1
        else:
            result.append(right[j])
            j += 1
    result.extend(left[i:])
    result.extend(right[j:])
    return result

arr = [5, 2, 4, 1, 3]
print(merge_sort(arr))

**Explanation of O(n log n):**

1. **log n levels** â†’ each recursion splits the array in half (`arr[:mid]` and `arr[mid:]`).
2. **n work at each level** â†’ merging all elements takes linear time.
3. **Total = n Ã— log n** â†’ O(n log n)



## ğŸ”¹ Real-Life Example in Python

* **`sorted()`** function in Python uses **Timsort** â†’ average case O(n log n).

```python
nums = [5, 2, 4, 1, 3]
sorted_nums = sorted(nums)  # internally O(n log n)
print(sorted_nums)
```


### ğŸ”¹ Why Time Complexity Matters

1. Helps us **predict performance** for large inputs.
2. Lets us **choose efficient algorithms**.
3. Avoids writing code that is **too slow** for big datasets.


Hereâ€™s a simple table for **5 common complexities**:

| Complexity | Best Case                  | Average Case  | Worst Case  | Example in Python            |
| ---------- | -------------------------- | ------------- | ----------- | ---------------------------- |
| O(1)       | Constant                   | Constant      | Constant    | Access list element `lst[0]` |
| O(log n)   | 1 step                     | log n steps   | log n steps | Binary search on sorted list |
| O(n)       | 1 step (found first)       | n/2 steps â‰ˆ n | n steps     | Linear search in list        |
| O(n log n) | n log n                    | n log n       | n log n     | Merge sort, Timsort          |
| O(nÂ²)      | 1 step (minimal iteration) | nÂ²/2 â‰ˆ nÂ²     | nÂ² steps    | Nested loops, bubble sort    |


### ğŸ”¹ Explanation:

1. **O(1)** â€“ Constant: Always the same time, doesnâ€™t depend on input size.
2. **O(log n)** â€“ Logarithmic: Each step halves the problem size (binary search).
3. **O(n)** â€“ Linear: Time grows proportionally to number of elements (linear search).
4. **O(n log n)** â€“ Linearithmic: Sorting algorithms like Merge Sort, Timsort.
5. **O(nÂ²)** â€“ Quadratic: Nested loops, comparing every pair of elements (Bubble sort).


